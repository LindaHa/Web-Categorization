{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import ast\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "\n",
    "input_file_relative = \"Datasets/Feature_dataset.csv\"\n",
    "output_file_relative = \"Datasets/hopefully_the_end.csv\"\n",
    "\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 100\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new generated data set file\n",
    "df = pd.read_csv(input_file_relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take only English documents - above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   main_category  main_category:confidence  \\\n",
      "23          Porn                         1   \n",
      "\n",
      "                                                  url  \\\n",
      "23  http://jvoa2vvavjxsd6sa5tviw5iy254czhyxnavxvxd...   \n",
      "\n",
      "                                              content  \\\n",
      "23  Tabu Child Porn CP # Tabu Child Porn cp    * H...   \n",
      "\n",
      "                                            tokens_en  english:confidence  \n",
      "23  ['tabu', 'child', 'porn', 'home', 'login', 'se...           57.017544  \n"
     ]
    }
   ],
   "source": [
    "def remove_non_english_documents(data_frame, english_tolerance = 20):\n",
    "    english_confidence = []\n",
    "    for i, doc in data_frame.iterrows():\n",
    "        english_words = 0\n",
    "        wordies = ast.literal_eval(doc['tokens_en'])\n",
    "        for w in wordies:\n",
    "            if w.lower() in english_vocab:\n",
    "                english_words += 1\n",
    "        english_confidence.append(english_words / len(wordies) * 100)\n",
    "    data_frame['english:confidence'] = english_confidence\n",
    "    \n",
    "    return data_frame[data_frame['english:confidence'] > english_tolerance]\n",
    "\n",
    "df = remove_non_english_documents(df)\n",
    "df = df.head(24)\n",
    "df = df.tail(1)\n",
    "pprint.pprint(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the most popular word list for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def most_popular_words_per_category(data_frame, top_number = top):\n",
    "    words_of_category = {}\n",
    "    for cat in set(data_frame['main_category'].values):\n",
    "        all_words = []\n",
    "        for wordies in data_frame[data_frame['main_category'] == cat]['tokens_en'].tolist():\n",
    "            for w in ast.literal_eval(wordies):\n",
    "                all_words.append(w)\n",
    "        all_word_except_stop_dist = nltk.FreqDist(\n",
    "            stemmer.stem(w.lower()) for w in all_words if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist\n",
    "        )\n",
    "    \n",
    "        most_common = all_word_except_stop_dist.most_common(top_number)\n",
    "        words_of_category[cat] = [w for w, number in most_common]\n",
    "    \n",
    "    return words_of_category\n",
    "\n",
    "words_frequency = most_popular_words_per_category(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation if it is not existing.\n",
    "__Dataset is filtered by these set of rules:__\n",
    "1. Main category != Not_working (Exclude non working URL's)\n",
    "2. Main category:confidence > 0.5 (Leave url's with likely know categories)\n",
    "4. Non english language URL's are excluded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove most frequent words in all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_clutter_words(words_per_category):\n",
    "    from flashtext.keyword import KeywordProcessor\n",
    "    from collections import Counter\n",
    "    wordies = []\n",
    "    for cat in words_per_category.keys():\n",
    "        wordies.extend(words_per_category[cat][0:15])\n",
    "    words_counter = Counter(wordies)\n",
    "    words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 7}\n",
    "    words_stop = list(words_filter.keys())\n",
    "    for cat in words_per_category.keys():\n",
    "        words_per_category[cat] = [w for w in words_per_category[cat] if w not in words_stop]\n",
    "        \n",
    "    return words_per_category\n",
    "\n",
    "words_frequency = remove_clutter_words(words_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get all top words of all categories combined\n",
    "### Create a keyword processor for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from flashtext.keyword import KeywordProcessor\n",
    "from collections import Counter\n",
    "all_keywords = []\n",
    "word_processors = {}\n",
    "for category in words_frequency.keys():\n",
    "    all_keywords.extend(words_frequency[category])\n",
    "    word_processor = KeywordProcessor()\n",
    "    for word in words_frequency[category]:\n",
    "        word_processor.add_keyword(word)\n",
    "    word_processors[category] = word_processor\n",
    "# remove duplicates    \n",
    "all_keywords = set(all_keywords)\n",
    "all_keywords = list(all_keywords)\n",
    "all_words_processor = KeywordProcessor()\n",
    "for word in all_keywords:\n",
    "    all_words_processor.add_keyword(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a percentage function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_percentage(dum0, dumx):\n",
    "#     try:\n",
    "#         ans=float(dumx)/float(dum0)\n",
    "#         ans=ans*100\n",
    "#     except:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return ans\n",
    "#    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to find the most probable category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "pycharm": {
     "name": "#%%   \n"
    }
   },
   "outputs": [],
   "source": [
    "# def guess_category(text, index):\n",
    "#     x=str(text)\n",
    "#     total_matches = len(all_words_processor.extract_keywords(x))\n",
    "#     if total_matches == 0:\n",
    "#         return 'Not working'\n",
    "#     \n",
    "#     matched_keywords_count = {}\n",
    "#     for p_key in word_processors:\n",
    "#         processor = word_processors[p_key]\n",
    "#         matched_keywords_count[p_key] = len(processor.extract_keywords(x))\n",
    "#         \n",
    "#     match_per_category = {}\n",
    "#     for tk_key in matched_keywords_count:\n",
    "#         matched = matched_keywords_count[tk_key]\n",
    "#         match_per_category[tk_key] = float(compute_percentage(total_matches, matched))\n",
    "#         \n",
    "#     max_prob_category = max(match_per_category, key=(lambda key: match_per_category[key]))\n",
    "#     return max_prob_category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each website use only its tokenized words and its category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [stemmer.stem(word.lower()) for word, word_count in Counter(ast.literal_eval(row['tokens_en'])).most_common(top)]\n",
    "    documents.append((c, row['main_category']))\n",
    "    \n",
    "words = all_keywords\n",
    "classes = list(words_frequency)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Language Words into mathematical notations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "pycharm": {
     "name": "#%%  \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nud'\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output = [] \n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current category\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "    \n",
    "pprint.pprint(all_keywords[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the final pre-processing on the data and create some functions:\n",
    "### Sigmoid Function\n",
    "### A function for cleaning up sentences\n",
    "### A function to create a Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    result = 1/(1+np.exp(-x))\n",
    "    return result\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(result):\n",
    "    return result*(1-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # get rid of non English words\n",
    "    english_words = []\n",
    "    for sw in sentence_words:\n",
    "        if sw.lower() in english_vocab:\n",
    "            english_words.append(sw)\n",
    "    # stem each word\n",
    "    stemmer = LancasterStemmer()\n",
    "    english_words = [stemmer.stem(sw.lower()) for sw in english_words]\n",
    "    return english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bow(sentence, wordies, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag_of_words = [0]*len(wordies)  \n",
    "    enumerated_words = enumerate(wordies)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerated_words:\n",
    "            if w == s: \n",
    "                bag_of_words[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag_of_words: %s\" % w)\n",
    "\n",
    "    return np.array(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (f\"Training with {hidden_neurons} neurons, alpha: {str(alpha)}, dropout:{dropout} {dropout_percent if dropout else ''}\")\n",
    "    print (f\"Input matrix: {len(X)}x{len(X[0])}    Output matrix: {1}x{ len(classes)}\")\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if dropout:\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if j > 0:\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "    folder_path = \"Models/\"\n",
    "    with open(folder_path+synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10 neurons, alpha: 0.1, dropout:False \n",
      "Input matrix: 1x100    Output matrix: 1x1\n",
      "delta after 10000 iterations:0.011372703734532918\n",
      "delta after 20000 iterations:0.00798591032204643\n",
      "delta after 30000 iterations:0.006501037764156448\n",
      "delta after 40000 iterations:0.005620483791714315\n",
      "delta after 50000 iterations:0.005021498395935131\n",
      "saved synapses to: synapses.json\n",
      "processing time: 3.391897678375244 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=10, alpha=0.1, epochs=50000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The helper function used in the neural network testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def think(sentence, synapses, show_details=False):\n",
    "    x = bow(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our bag of words\n",
    "    layer_0 = x\n",
    "    synapse_0 = synapses[0]\n",
    "    synapse_1 = synapses[1]\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "    # output layer\n",
    "    layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Porn', 0.8395964685686156]]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# load our calculated synapse values\n",
    "syn_file = 'Models/synapses.json' \n",
    "with open(syn_file) as data_file: \n",
    "    syn = json.load(data_file) \n",
    "    syn_0 = np.asarray(syn['synapse0']) \n",
    "    syn_1 = np.asarray(syn['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = think(sentence, (syn_0, syn_1), show_details)\n",
    "\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    #print (\"\\n classification: %s\" % ( return_results))\n",
    "    return return_results\n",
    "\n",
    "test_text = '56'\n",
    "classify(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
