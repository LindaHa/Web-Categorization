{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "\n",
    "input_file_relative = \"Datasets/Dark_web_dataset_enhanced.csv\"\n",
    "output_file_relative = \"Datasets/hopefully_the_end.csv\"\n",
    "\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 1000\n",
    "MAX_SEQUENCE_LENGTH = top\n",
    "MAX_NB_WORDS = top\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "all_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read new generated data set file and set en_tokens to ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4334 rows in 4 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file_relative)[['url', 'content', 'category']]\n",
    "df['tokens_en', 'confidence'] = ''\n",
    "# Shuffle the rows and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(f\"Loaded {df.shape[0]} rows in {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take only English documents - above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_english_documents(data_frame, english_tolerance = 20):\n",
    "    removed = 0\n",
    "    english_confidence = []\n",
    "    tokens_en = []\n",
    "    for i, document in data_frame.iterrows():\n",
    "        english_words = 0\n",
    "        text = document['content']\n",
    "        \n",
    "        # Remove long base-64 encoded strings, e.g. images\n",
    "        text = re.sub(\"(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=|[A-Za-z0-9+/]{4}){24,}\",\"\", text)\n",
    "        \n",
    "        wordies = nltk.word_tokenize(text)\n",
    "\n",
    "        tokens = []\n",
    "        for w in wordies:\n",
    "            lower = w.lower()\n",
    "            if lower in english_vocab:\n",
    "                tokens.append(lower)\n",
    "                english_words += 1\n",
    "        tokens_en.append(tokens)\n",
    "        doc_english_confidence = english_words / len(wordies) * 100\n",
    "        english_confidence.append(doc_english_confidence)\n",
    "        if doc_english_confidence <= english_tolerance:\n",
    "            removed += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"done {}\".format(i))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    data_frame['english:confidence'] = english_confidence\n",
    "    data_frame['tokens_en'] = tokens_en\n",
    "    print(f\"Removed {removed} documents considered non-english.\")\n",
    "    return data_frame[data_frame['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 100\n",
      "done 200\n",
      "done 300\n",
      "done 400\n",
      "done 500\n",
      "done 600\n",
      "done 700\n",
      "done 800\n",
      "done 900\n",
      "done 1000\n",
      "done 1100\n",
      "done 1200\n",
      "done 1300\n",
      "done 1400\n",
      "done 1500\n",
      "done 1600\n",
      "done 1700\n",
      "done 1800\n",
      "done 1900\n",
      "done 2000\n",
      "done 2100\n",
      "done 2200\n",
      "done 2300\n",
      "done 2400\n",
      "done 2500\n",
      "done 2600\n",
      "done 2700\n",
      "done 2800\n",
      "done 2900\n",
      "done 3000\n",
      "done 3100\n",
      "done 3200\n",
      "done 3300\n",
      "done 3400\n",
      "done 3500\n",
      "done 3600\n",
      "done 3700\n",
      "done 3800\n",
      "done 3900\n",
      "done 4000\n",
      "done 4100\n",
      "done 4200\n",
      "done 4300\n",
      "Removed 13 documents considered non-english.\n"
     ]
    }
   ],
   "source": [
    "df = remove_non_english_documents(df)\n",
    "#df = pd.read_csv('Datasets/english_tokens.csv')[['url', 'content', 'category', 'tokens_en', 'english:confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get labels\n",
    "# The labels array is a lookup for label (category) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deviancy': 4,\n",
      " 'Finance and Fraud': 1,\n",
      " 'Gambling': 7,\n",
      " 'Hosting and Programming and Hacking': 5,\n",
      " 'Illegal services and goods': 6,\n",
      " 'Online Marketplace': 8,\n",
      " 'Other': 3,\n",
      " 'Porn': 0,\n",
      " 'Social': 9,\n",
      " 'Web Catalogue': 2}\n"
     ]
    }
   ],
   "source": [
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "def get_labels(d_frame):\n",
    "    data_frame = d_frame.copy(deep = True)\n",
    "    df_categories = data_frame.drop_duplicates(subset = 'category')\n",
    "    df_categories = df_categories['category']\n",
    "    \n",
    "    index = 0\n",
    "    for category in df_categories:\n",
    "        labels_index[category] = index\n",
    "        index += 1\n",
    "\n",
    "get_labels(df)\n",
    "pprint.pprint(labels_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the texts array with the raw content of every page\n",
    "# Fill the labels_index array with the category id on the index position of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frigid Bdsm Porno Skachat : Immaturely Aglitter Gianna Michaels Porn Videos '\n",
      " 'Not To Mention Best Blonde Free Porn Laigh : Tube Porno Hotel   * OnionLand '\n",
      " \"Search  This is OnionLand Search's cache of http://studentki-na- \"\n",
      " 'porno.darkdirzzhtm3pphblyaxkpggn5yki64hckrvokk2u6asdd7d2tl2nad.onion/www- '\n",
      " 'porno-video-besplatno/bald-headed-porn-woman.html. It is a snapshot of the '\n",
      " 'page as it appeared on Oct 17, 2019 14:41:06. The current page could have '\n",
      " 'changed in the meantime.   **Full version**    Text-only version    View '\n",
      " 'source  * * *  * * *  ![](/tr?id=21r3ZarDVb&t=ZnjYEgdm)  * * *  '\n",
      " '**Explore**    * Discover Dark Web Hidden Service   * Most Popular   * '\n",
      " 'Random Onion  **Products**    * Deep Web Advertisement   * Deep Web '\n",
      " 'Hosting   * Deep Web Domain  **Onionland**    * Add Service   * Contact   * '\n",
      " 'Cookie Policy   * Disclaimer   * Terms of Use  OnionLand Search (C) 2019   '\n",
      " 'Onion Service: http://3bbaaaccczcbdddz.onion    Onion Service V3: '\n",
      " 'http://3bbad7fauom4d6sgppalyqddsqbf5u5p56b5k5uk2zxsy3d6ey2jobad.onion  '\n",
      " 'Clearnet Service: https://onionlandsearchengine.com  '\n",
      " '![](http://dcounter3sjplzorng2oekjmt6xdaa5qkdc3guypuqranjjeoxlgc4id.onion/counter.gif?id=88987e343fca6edb5460351b3085b43b&bg=000000&fg=FFFFFF&tr=0&unique=0&mode=0)  ']\n",
      "[0, 1, 2, 0, 0]\n",
      "4321\n",
      "4321\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "def prepare_text_data(data_frame):\n",
    "    for i, document in data_frame.iterrows():\n",
    "        text = document['content']\n",
    "        texts.append(text)\n",
    "        category = document['category']\n",
    "        label = labels_index[category]\n",
    "        labels.append(label)\n",
    "\n",
    "prepare_text_data(df)\n",
    "\n",
    "pprint.pprint(texts[:1])    \n",
    "pprint.pprint(labels[:5])\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n",
      "        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n",
      "        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n",
      "        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n",
      "       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n",
      "        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n",
      "        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n",
      "        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n",
      "        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n",
      "       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n",
      "       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n",
      "       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n",
      "       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n",
      "        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n",
      "       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n",
      "        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n",
      "        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n",
      "       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n",
      "        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n",
      "       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "pprint.pprint(embeddings_index[\"hello\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nefarion/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 129989 unique tokens.\n",
      "Shape of data tensor: (4321, 1000)\n",
      "Shape of label tensor: (4321, 10)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"could not find the word {} in the embeddings dictionary\".format(word))\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "use_glove_embeddings = False #it seems that using glove embeddings doesn't bring up any noticeable improvement\n",
    "if use_glove_embeddings:\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "else:\n",
    "    embedding_layer = Embedding(num_words,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "10\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,246,026\n",
      "Trainable params: 2,246,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3457 samples, validate on 864 samples\n",
      "Epoch 1/20\n",
      "3457/3457 [==============================] - 30s 9ms/step - loss: 1.6076 - acc: 0.4623 - val_loss: 1.7116 - val_acc: 0.4606\n",
      "Epoch 2/20\n",
      "3457/3457 [==============================] - 30s 9ms/step - loss: 1.1219 - acc: 0.6552 - val_loss: 1.8656 - val_acc: 0.4769\n",
      "Epoch 3/20\n",
      "3457/3457 [==============================] - 30s 9ms/step - loss: 0.8823 - acc: 0.7394 - val_loss: 1.1203 - val_acc: 0.6921\n",
      "Epoch 4/20\n",
      "3457/3457 [==============================] - 30s 9ms/step - loss: 0.7341 - acc: 0.7796 - val_loss: 1.4529 - val_acc: 0.4931\n",
      "Epoch 5/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.6722 - acc: 0.7964 - val_loss: 1.4512 - val_acc: 0.6262\n",
      "Epoch 6/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.5922 - acc: 0.8215 - val_loss: 0.6616 - val_acc: 0.8183\n",
      "Epoch 7/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.5170 - acc: 0.8461 - val_loss: 1.7350 - val_acc: 0.5521\n",
      "Epoch 8/20\n",
      "3457/3457 [==============================] - 30s 9ms/step - loss: 0.4785 - acc: 0.8588 - val_loss: 0.7803 - val_acc: 0.8032\n",
      "Epoch 9/20\n",
      "3457/3457 [==============================] - 29s 8ms/step - loss: 0.4099 - acc: 0.8811 - val_loss: 0.6721 - val_acc: 0.8287\n",
      "Epoch 10/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.3550 - acc: 0.9002 - val_loss: 1.4214 - val_acc: 0.6262\n",
      "Epoch 11/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.3355 - acc: 0.9066 - val_loss: 0.7733 - val_acc: 0.8113\n",
      "Epoch 12/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.3031 - acc: 0.9176 - val_loss: 1.0992 - val_acc: 0.7812\n",
      "Epoch 13/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.2835 - acc: 0.9231 - val_loss: 0.6721 - val_acc: 0.8611\n",
      "Epoch 14/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.2563 - acc: 0.9274 - val_loss: 0.6898 - val_acc: 0.8553\n",
      "Epoch 15/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.2446 - acc: 0.9326 - val_loss: 0.7610 - val_acc: 0.8507\n",
      "Epoch 16/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.2177 - acc: 0.9398 - val_loss: 0.7525 - val_acc: 0.8484\n",
      "Epoch 17/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.2275 - acc: 0.9349 - val_loss: 0.7201 - val_acc: 0.8530\n",
      "Epoch 18/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.1782 - acc: 0.9552 - val_loss: 1.5660 - val_acc: 0.7095\n",
      "Epoch 19/20\n",
      "3457/3457 [==============================] - 28s 8ms/step - loss: 0.1827 - acc: 0.9543 - val_loss: 0.9228 - val_acc: 0.8391\n",
      "Epoch 20/20\n",
      "3457/3457 [==============================] - 27s 8ms/step - loss: 0.1538 - acc: 0.9589 - val_loss: 0.8284 - val_acc: 0.8576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8defa41a20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training model.')\n",
    "print(len(labels_index))\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data of 864 examples\n",
      "864/864 [==============================] - 2s 2ms/step\n",
      "test loss, test acc: [0.8283674628646286, 0.8576388955116272]\n",
      "\n",
      "# Detailed results for training data set\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Gambling should have been Other\n",
      "Predicted Other should have been Illegal services and goods\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Other should have been Social\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Other should have been Web Catalogue\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Illegal services and goods should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Finance and Fraud\n",
      "Predicted Social should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Illegal services and goods\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Porn\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Porn\n",
      "Predicted Social should have been Porn\n",
      "Predicted Porn should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Other should have been Deviancy\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Porn should have been Other\n",
      "Predicted Web Catalogue should have been Illegal services and goods\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Social\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Illegal services and goods should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Porn should have been Other\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Illegal services and goods\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Illegal services and goods\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Other should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Illegal services and goods should have been Online Marketplace\n",
      "Predicted Other should have been Social\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Other should have been Social\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Illegal services and goods\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Social should have been Web Catalogue\n",
      "Predicted Hosting and Programming and Hacking should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Social\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Social should have been Deviancy\n",
      "Predicted Hosting and Programming and Hacking should have been Porn\n",
      "Predicted Porn should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Finance and Fraud should have been Gambling\n",
      "Predicted Web Catalogue should have been Social\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Illegal services and goods should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Web Catalogue should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Other\n",
      "Predicted Social should have been Illegal services and goods\n",
      "Predicted Web Catalogue should have been Social\n",
      "Predicted Porn should have been Other\n",
      "Predicted Other should have been Porn\n",
      "Predicted Social should have been Porn\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Finance and Fraud should have been Illegal services and goods\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Online Marketplace should have been Finance and Fraud\n",
      "Predicted Social should have been Illegal services and goods\n",
      "Predicted Gambling should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Finance and Fraud should have been Illegal services and goods\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Illegal services and goods should have been Finance and Fraud\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Porn should have been Deviancy\n",
      "\n",
      "# Generate predictions for custom sample\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data of {} examples'.format(len(x_val)))\n",
    "results = model.evaluate(x_val, y_val, batch_size=128)\n",
    "run_accuracy = results[1]\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "category_id_to_name_lookup = {v: k for k, v in labels_index.items()}\n",
    "\n",
    "print('\\n# Detailed results for training data set')\n",
    "predictions = model.predict(x_val)\n",
    "predictions_copy = predictions.copy()\n",
    "for idx, val in enumerate(predictions):\n",
    "    category_index = predictions[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    if original_category_index != category_index:\n",
    "        print(\"Predicted {} should have been {}\"\n",
    "              .format(category_id_to_name_lookup[category_index], category_id_to_name_lookup[original_category_index]))\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for custom sample')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way — the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too — not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.\n",
      "\n",
      "# Saving model to disk\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(\"\\n# Saving model to disk\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/DarkWebCategoryModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Models/DarkWebCategoryModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Test loaded model and tokenizer\n",
      "\n",
      "# Loading Tokenizer\n",
      "\n",
      "# Loading saved Model\n",
      "\n",
      "# Loading saved Model Weights\n",
      "\n",
      "# Compiling loaded Model\n",
      "Loading tokenizer, model, weights and compiling it took 0.6892285346984863 seconds.\n",
      "\n",
      "# Use loaded model on a custom exmaple\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "print('\\n# Test loaded model and tokenizer')\n",
    "\n",
    "print('\\n# Loading Tokenizer')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "    \n",
    "print('\\n# Loading saved Model')\n",
    "from keras.models import model_from_json\n",
    "json_file = open('Models/DarkWebCategoryModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "print('\\n# Loading saved Model Weights')\n",
    "loaded_model.load_weights(\"Models/DarkWebCategoryModel.h5\")\n",
    "print('\\n# Compiling loaded Model')\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(\"Loading tokenizer, model, weights and compiling it took {} seconds.\".format(time.time() - start))\n",
    " \n",
    "print('\\n# Use loaded model on a custom exmaple')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way — the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too — not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = loaded_tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = loaded_model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8576388955116272\n",
      "                                   : ['Art        ', 'Deviancy   ', 'Drugs      ', 'Encyklo    ', 'FakeIdnHitm', 'FinFraud   ', 'Gambling   ', 'Guns       ', 'HacProgr   ', 'OnlnMarket ', 'Other      ', 'Porn       ', 'Social     ', 'WebCat     ']\n",
      "\n",
      "Deviancy                           : ['12: 54.55% ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 4.55%   ', '8: 36.36%  ', '1: 4.55%   ', '0: 0.00%   ']\n",
      "\n",
      "Finance and Fraud                  : ['0: 0.00%   ', '129: 94.16%', '0: 0.00%   ', '1: 0.73%   ', '1: 0.73%   ', '1: 0.73%   ', '2: 1.46%   ', '0: 0.00%   ', '1: 0.73%   ', '2: 1.46%   ']\n",
      "\n",
      "Gambling                           : ['0: 0.00%   ', '1: 2.44%   ', '40: 97.56% ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ']\n",
      "\n",
      "Hosting and Programming and Hacking: ['0: 0.00%   ', '3: 3.53%   ', '0: 0.00%   ', '65: 76.47% ', '1: 1.18%   ', '0: 0.00%   ', '7: 8.24%   ', '1: 1.18%   ', '6: 7.06%   ', '2: 2.35%   ']\n",
      "\n",
      "Illegal services and goods         : ['0: 0.00%   ', '2: 7.41%   ', '0: 0.00%   ', '2: 7.41%   ', '17: 62.96% ', '0: 0.00%   ', '2: 7.41%   ', '0: 0.00%   ', '3: 11.11%  ', '1: 3.70%   ']\n",
      "\n",
      "Online Marketplace                 : ['0: 0.00%   ', '3: 10.00%  ', '0: 0.00%   ', '2: 6.67%   ', '1: 3.33%   ', '23: 76.67% ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 3.33%   ']\n",
      "\n",
      "Other                              : ['0: 0.00%   ', '6: 3.82%   ', '2: 1.27%   ', '6: 3.82%   ', '0: 0.00%   ', '0: 0.00%   ', '133: 84.71%', '4: 2.55%   ', '2: 1.27%   ', '4: 2.55%   ']\n",
      "\n",
      "Porn                               : ['3: 1.32%   ', '1: 0.44%   ', '0: 0.00%   ', '2: 0.88%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 0.44%   ', '218: 95.61%', '3: 1.32%   ', '0: 0.00%   ']\n",
      "\n",
      "Social                             : ['0: 0.00%   ', '4: 10.26%  ', '0: 0.00%   ', '11: 28.21% ', '2: 5.13%   ', '0: 0.00%   ', '5: 12.82%  ', '0: 0.00%   ', '14: 35.90% ', '3: 7.69%   ']\n",
      "\n",
      "Web Catalogue                      : ['0: 0.00%   ', '2: 2.04%   ', '0: 0.00%   ', '1: 1.02%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 1.02%   ', '3: 3.06%   ', '1: 1.02%   ', '90: 91.84% ']\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = [cat_key for cat_key in labels_index]\n",
    "sorted_labels.sort()\n",
    "# Initiate confusion matrix\n",
    "matrix = {cat_key: {} for cat_key in sorted_labels}\n",
    "for row in matrix:\n",
    "    matrix[row] = {cat_key: 0 for cat_key in sorted_labels}\n",
    "\n",
    "len_of_longest = 0\n",
    "for label in sorted_labels:\n",
    "    label_len = len(label)\n",
    "    len_of_longest = label_len if label_len > len_of_longest else len_of_longest\n",
    "       \n",
    "# Create confusion matrix\n",
    "for idx, val in enumerate(predictions_copy):\n",
    "    category_index = predictions_copy[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    category = category_id_to_name_lookup[category_index]\n",
    "    original_category = category_id_to_name_lookup[original_category_index]\n",
    "    matrix[original_category][category] += 1\n",
    "\n",
    "# Simplify confusion matrix\n",
    "def get_percentage(res, values_sum):\n",
    "    return f'{res/values_sum*100:.2f}' if values_sum else '0.00'\n",
    "\n",
    "simple_matrix = matrix.copy()\n",
    "for row in simple_matrix:\n",
    "    values = simple_matrix[row].values()\n",
    "    values_sum = sum(values)\n",
    "    results_row = [ '{:11}'.format(f'{res}: {get_percentage(res, values_sum)}%') for res in list(values) ]\n",
    "    beau_results = [res.replace('\"', '') for res in results_row]\n",
    "    simple_matrix[row] = beau_results\n",
    "        \n",
    "# Print confusion matrix\n",
    "print(f'accuracy: {run_accuracy}')\n",
    "short_labels = ['OnlnMarket', 'WebCat', 'Porn', 'FinFraud', 'Other', 'Social', 'HacProgr', 'Deviancy', 'Gambling', 'IllegalServ']\n",
    "short_labels.sort()\n",
    "formatted_short_labels = ['{:11}'.format(lab) for lab in short_labels]\n",
    "print(\"{:{len_of_longest}}: {}\".format(' ', formatted_short_labels, len_of_longest=len_of_longest))\n",
    "for row in simple_matrix:\n",
    "    print(\"\\n{:{len_of_longest}}: {}\".format(row, simple_matrix[row], len_of_longest=len_of_longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}