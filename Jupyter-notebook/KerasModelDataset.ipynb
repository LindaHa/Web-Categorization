{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import ast\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "\n",
    "input_file_relative = \"Datasets/Dark_web_dataset - Copy (2).csv\"\n",
    "output_file_relative = \"Datasets/hopefully_the_end.csv\"\n",
    "\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 1000\n",
    "MAX_SEQUENCE_LENGTH = top\n",
    "MAX_NB_WORDS = top\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "all_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read new generated data set file and set en_tokens to ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4120 rows in 4 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file_relative)[['url', 'content', 'category']]\n",
    "df['tokens_en', 'confidence'] = ''\n",
    "# Shuffle the rows and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(f\"Loaded {df.shape[0]} rows in {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take only English documents - above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_english_documents(data_frame, english_tolerance = 20):\n",
    "    removed = 0\n",
    "    english_confidence = []\n",
    "    tokens_en = []\n",
    "    for i, document in data_frame.iterrows():\n",
    "        english_words = 0\n",
    "        text = document['content']\n",
    "        \n",
    "        # Remove long base-64 encoded strings, e.g. images\n",
    "        text = re.sub(\"(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=|[A-Za-z0-9+/]{4}){24,}\",\"\", text)\n",
    "        \n",
    "        wordies = nltk.word_tokenize(text)\n",
    "\n",
    "        tokens = []\n",
    "        for w in wordies:\n",
    "            lower = w.lower()\n",
    "            if lower in english_vocab:\n",
    "                tokens.append(lower)\n",
    "                english_words += 1\n",
    "        tokens_en.append(tokens)\n",
    "        doc_english_confidence = english_words / len(wordies) * 100\n",
    "        english_confidence.append(doc_english_confidence)\n",
    "        if doc_english_confidence <= english_tolerance:\n",
    "            removed += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"done {}\".format(i))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    data_frame['english:confidence'] = english_confidence\n",
    "    data_frame['tokens_en'] = tokens_en\n",
    "    print(f\"Removed {removed} documents considered non-english.\")\n",
    "    return data_frame[data_frame['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 100\n",
      "done 200\n",
      "done 300\n",
      "done 400\n",
      "done 500\n",
      "done 600\n",
      "done 700\n",
      "done 800\n",
      "done 900\n",
      "done 1000\n",
      "done 1100\n",
      "done 1200\n",
      "done 1300\n",
      "done 1400\n",
      "done 1500\n",
      "done 1600\n",
      "done 1700\n",
      "done 1800\n",
      "done 1900\n",
      "done 2000\n",
      "done 2100\n",
      "done 2200\n",
      "done 2300\n",
      "done 2400\n",
      "done 2500\n",
      "done 2600\n",
      "done 2700\n",
      "done 2800\n",
      "done 2900\n",
      "done 3000\n",
      "done 3100\n",
      "done 3200\n",
      "done 3300\n",
      "done 3400\n",
      "done 3500\n",
      "done 3600\n",
      "done 3700\n",
      "done 3800\n",
      "done 3900\n",
      "done 4000\n",
      "done 4100\n",
      "Removed 13 documents considered non-english.\n"
     ]
    }
   ],
   "source": [
    "df = remove_non_english_documents(df)\n",
    "#df = pd.read_csv('Datasets/english_tokens.csv')[['url', 'content', 'category', 'tokens_en', 'english:confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get labels\n",
    "# The labels array is a lookup for label (category) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Art': 6,\n",
      " 'Deviancy': 7,\n",
      " 'Drugs': 9,\n",
      " 'Encyclopedia and Knowledge': 11,\n",
      " 'Fake Identity and Hitmen': 10,\n",
      " 'Finance and Fraud': 0,\n",
      " 'Financial Fraud': 13,\n",
      " 'Gambling': 12,\n",
      " 'Guns': 14,\n",
      " 'Hosting and Programming and Hacking': 5,\n",
      " 'Online Marketplace': 8,\n",
      " 'Other': 1,\n",
      " 'Porn': 2,\n",
      " 'Social': 4,\n",
      " 'Web Catalogue': 3}\n"
     ]
    }
   ],
   "source": [
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "def get_labels(d_frame):\n",
    "    data_frame = d_frame.copy(deep = True)\n",
    "    df_categories = data_frame.drop_duplicates(subset = 'category')\n",
    "    df_categories = df_categories['category']\n",
    "    \n",
    "    index = 0\n",
    "    for category in df_categories:\n",
    "        labels_index[category] = index\n",
    "        index += 1\n",
    "\n",
    "get_labels(df)\n",
    "pprint.pprint(labels_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the texts array with the raw content of every page\n",
    "# Fill the labels_index array with the category id on the index position of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.L33T Shop. Carding CC/DUMPS/FULLZ ![](img.2/logo.login.png)      ---      '\n",
      " '### USERNAME  |      ### PASSWORD  |   | '\n",
      " '![](/captcha/captcha.php?0.19272900+1571772442)      ### CAPTCHA  |   |  '\n",
      " 'LOGIN NOW   |  #### If you still do not have a user account, you can create '\n",
      " 'one by clicking here      **L33T Networks** 2009-2016  '\n",
      " '3s2v3uhkxr4pfoa3.onion  ']\n",
      "[0, 1, 2, 2, 3]\n",
      "4107\n",
      "4107\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "def prepare_text_data(data_frame):\n",
    "    for i, document in data_frame.iterrows():\n",
    "        text = document['content']\n",
    "        texts.append(text)\n",
    "        category = document['category']\n",
    "        label = labels_index[category]\n",
    "        labels.append(label)\n",
    "\n",
    "prepare_text_data(df)\n",
    "\n",
    "pprint.pprint(texts[:1])    \n",
    "pprint.pprint(labels[:5])\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n",
      "        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n",
      "        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n",
      "        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n",
      "       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n",
      "        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n",
      "        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n",
      "        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n",
      "        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n",
      "       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n",
      "       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n",
      "       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n",
      "       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n",
      "        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n",
      "       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n",
      "        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n",
      "        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n",
      "       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n",
      "        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n",
      "       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "pprint.pprint(embeddings_index[\"hello\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nefarion/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 129766 unique tokens.\n",
      "Shape of data tensor: (4107, 1000)\n",
      "Shape of label tensor: (4107, 15)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"could not find the word {} in the embeddings dictionary\".format(word))\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "use_glove_embeddings = False #it seems that using glove embeddings doesn't bring up any noticeable improvement\n",
    "if use_glove_embeddings:\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "else:\n",
    "    embedding_layer = Embedding(num_words,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "15\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 2,246,671\n",
      "Trainable params: 2,246,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3286 samples, validate on 821 samples\n",
      "Epoch 1/20\n",
      "3286/3286 [==============================] - 26s 8ms/step - loss: 1.6762 - acc: 0.4702 - val_loss: 1.3137 - val_acc: 0.6078\n",
      "Epoch 2/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 1.1711 - acc: 0.6482 - val_loss: 1.0397 - val_acc: 0.6809\n",
      "Epoch 3/20\n",
      "3286/3286 [==============================] - 23s 7ms/step - loss: 0.9560 - acc: 0.7161 - val_loss: 0.8944 - val_acc: 0.7284\n",
      "Epoch 4/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.8345 - acc: 0.7538 - val_loss: 0.8707 - val_acc: 0.7747\n",
      "Epoch 5/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.7474 - acc: 0.7891 - val_loss: 0.8601 - val_acc: 0.7625\n",
      "Epoch 6/20\n",
      "3286/3286 [==============================] - 23s 7ms/step - loss: 0.6643 - acc: 0.8162 - val_loss: 0.7407 - val_acc: 0.7929\n",
      "Epoch 7/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.6050 - acc: 0.8244 - val_loss: 0.7896 - val_acc: 0.7759\n",
      "Epoch 8/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.5391 - acc: 0.8524 - val_loss: 0.7134 - val_acc: 0.7966\n",
      "Epoch 9/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.5401 - acc: 0.8472 - val_loss: 0.6683 - val_acc: 0.8210\n",
      "Epoch 10/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.4512 - acc: 0.8722 - val_loss: 0.8078 - val_acc: 0.7917\n",
      "Epoch 11/20\n",
      "3286/3286 [==============================] - 25s 8ms/step - loss: 0.4323 - acc: 0.8774 - val_loss: 0.7240 - val_acc: 0.8258\n",
      "Epoch 12/20\n",
      "3286/3286 [==============================] - 25s 8ms/step - loss: 0.3835 - acc: 0.8895 - val_loss: 0.6915 - val_acc: 0.8319\n",
      "Epoch 13/20\n",
      "3286/3286 [==============================] - 25s 8ms/step - loss: 0.3702 - acc: 0.8977 - val_loss: 0.7385 - val_acc: 0.8185\n",
      "Epoch 14/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.3456 - acc: 0.9072 - val_loss: 0.7599 - val_acc: 0.8185\n",
      "Epoch 15/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.2902 - acc: 0.9200 - val_loss: 0.9972 - val_acc: 0.7808\n",
      "Epoch 16/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.2847 - acc: 0.9221 - val_loss: 0.7515 - val_acc: 0.8197\n",
      "Epoch 17/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.2447 - acc: 0.9343 - val_loss: 0.8502 - val_acc: 0.7820\n",
      "Epoch 18/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.2389 - acc: 0.9385 - val_loss: 0.8394 - val_acc: 0.8307\n",
      "Epoch 19/20\n",
      "3286/3286 [==============================] - 25s 7ms/step - loss: 0.2292 - acc: 0.9315 - val_loss: 0.9832 - val_acc: 0.7686\n",
      "Epoch 20/20\n",
      "3286/3286 [==============================] - 24s 7ms/step - loss: 0.1990 - acc: 0.9425 - val_loss: 0.8394 - val_acc: 0.8283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f4b38fea898>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training model.')\n",
    "print(len(labels_index))\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data of 821 examples\n",
      "821/821 [==============================] - 2s 2ms/step\n",
      "test loss, test acc: [0.8394051143370941, 0.828258216381073]\n",
      "\n",
      "# Detailed results for training data set\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Other should have been Online Marketplace\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Deviancy should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Online Marketplace should have been Web Catalogue\n",
      "Predicted Social should have been Other\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Other should have been Social\n",
      "Predicted Online Marketplace should have been Finance and Fraud\n",
      "Predicted Porn should have been Other\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Other should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Other should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Fake Identity and Hitmen\n",
      "Predicted Social should have been Web Catalogue\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Drugs should have been Guns\n",
      "Predicted Social should have been Web Catalogue\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Web Catalogue should have been Finance and Fraud\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Art\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Other should have been Porn\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Finance and Fraud should have been Drugs\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Drugs should have been Encyclopedia and Knowledge\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Online Marketplace should have been Finance and Fraud\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Other should have been Social\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Other should have been Drugs\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Social should have been Fake Identity and Hitmen\n",
      "Predicted Drugs should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Web Catalogue\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Web Catalogue\n",
      "Predicted Social should have been Online Marketplace\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Social should have been Other\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Drugs should have been Guns\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Other should have been Fake Identity and Hitmen\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Finance and Fraud should have been Gambling\n",
      "Predicted Other should have been Deviancy\n",
      "Predicted Social should have been Fake Identity and Hitmen\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Porn should have been Other\n",
      "Predicted Porn should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Other\n",
      "Predicted Social should have been Guns\n",
      "Predicted Online Marketplace should have been Finance and Fraud\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Social should have been Guns\n",
      "Predicted Hosting and Programming and Hacking should have been Art\n",
      "Predicted Other should have been Social\n",
      "Predicted Social should have been Drugs\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Other should have been Social\n",
      "Predicted Finance and Fraud should have been Drugs\n",
      "Predicted Social should have been Fake Identity and Hitmen\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Social should have been Online Marketplace\n",
      "Predicted Other should have been Deviancy\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Other should have been Finance and Fraud\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Hosting and Programming and Hacking should have been Fake Identity and Hitmen\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Fake Identity and Hitmen should have been Drugs\n",
      "Predicted Other should have been Porn\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Fake Identity and Hitmen\n",
      "Predicted Porn should have been Deviancy\n",
      "\n",
      "# Generate predictions for custom sample\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data of {} examples'.format(len(x_val)))\n",
    "results = model.evaluate(x_val, y_val, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "category_id_to_name_lookup = {v: k for k, v in labels_index.items()}\n",
    "\n",
    "print('\\n# Detailed results for training data set')\n",
    "predictions = model.predict(x_val)\n",
    "for idx, val in enumerate(predictions):\n",
    "    category_index = predictions[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    if original_category_index != category_index:\n",
    "        print(\"Predicted {} should have been {}\"\n",
    "              .format(category_id_to_name_lookup[category_index], category_id_to_name_lookup[original_category_index]))\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for custom sample')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way — the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too — not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.\n",
      "\n",
      "# Saving model to disk\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(\"\\n# Saving model to disk\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/DarkWebCategoryModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Models/DarkWebCategoryModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Test loaded model and tokenizer\n",
      "\n",
      "# Loading Tokenizer\n",
      "\n",
      "# Loading saved Model\n",
      "\n",
      "# Loading saved Model Weights\n",
      "\n",
      "# Compiling loaded Model\n",
      "Loading tokenizer, model, weights and compiling it took 0.8846375942230225 seconds.\n",
      "\n",
      "# Use loaded model on a custom exmaple\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "print('\\n# Test loaded model and tokenizer')\n",
    "\n",
    "print('\\n# Loading Tokenizer')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "    \n",
    "print('\\n# Loading saved Model')\n",
    "from keras.models import model_from_json\n",
    "json_file = open('Models/DarkWebCategoryModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "print('\\n# Loading saved Model Weights')\n",
    "loaded_model.load_weights(\"Models/DarkWebCategoryModel.h5\")\n",
    "print('\\n# Compiling loaded Model')\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(\"Loading tokenizer, model, weights and compiling it took {} seconds.\".format(time.time() - start))\n",
    " \n",
    "print('\\n# Use loaded model on a custom exmaple')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way — the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too — not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = loaded_tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = loaded_model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
