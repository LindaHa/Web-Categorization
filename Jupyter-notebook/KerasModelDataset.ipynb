{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import ast\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "\n",
    "input_file_relative = \"Datasets/Dark_web_dataset - Copy (2).csv\"\n",
    "output_file_relative = \"Datasets/hopefully_the_end.csv\"\n",
    "\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 1000\n",
    "MAX_SEQUENCE_LENGTH = top\n",
    "MAX_NB_WORDS = top\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "all_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read new generated data set file and set en_tokens to ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4120 rows in 4 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file_relative)[['url', 'content', 'category']]\n",
    "df['tokens_en', 'confidence'] = ''\n",
    "# Shuffle the rows and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(f\"Loaded {df.shape[0]} rows in {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take only English documents - above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_english_documents(data_frame, english_tolerance = 20):\n",
    "    removed = 0\n",
    "    english_confidence = []\n",
    "    tokens_en = []\n",
    "    for i, document in data_frame.iterrows():\n",
    "        english_words = 0\n",
    "        text = document['content']\n",
    "        \n",
    "        # Remove long base-64 encoded strings, e.g. images\n",
    "        text = re.sub(\"(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=|[A-Za-z0-9+/]{4}){24,}\",\"\", text)\n",
    "        \n",
    "        wordies = nltk.word_tokenize(text)\n",
    "\n",
    "        tokens = []\n",
    "        for w in wordies:\n",
    "            lower = w.lower()\n",
    "            if lower in english_vocab:\n",
    "                tokens.append(lower)\n",
    "                english_words += 1\n",
    "        tokens_en.append(tokens)\n",
    "        doc_english_confidence = english_words / len(wordies) * 100\n",
    "        english_confidence.append(doc_english_confidence)\n",
    "        if doc_english_confidence <= english_tolerance:\n",
    "            removed += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"done {}\".format(i))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    data_frame['english:confidence'] = english_confidence\n",
    "    data_frame['tokens_en'] = tokens_en\n",
    "    print(f\"Removed {removed} documents considered non-english.\")\n",
    "    return data_frame[data_frame['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 100\n",
      "done 200\n",
      "done 300\n",
      "done 400\n",
      "done 500\n",
      "done 600\n",
      "done 700\n",
      "done 800\n",
      "done 900\n",
      "done 1000\n",
      "done 1100\n",
      "done 1200\n",
      "done 1300\n",
      "done 1400\n",
      "done 1500\n",
      "done 1600\n",
      "done 1700\n",
      "done 1800\n",
      "done 1900\n",
      "done 2000\n",
      "done 2100\n",
      "done 2200\n",
      "done 2300\n",
      "done 2400\n",
      "done 2500\n",
      "done 2600\n",
      "done 2700\n",
      "done 2800\n",
      "done 2900\n",
      "done 3000\n",
      "done 3100\n",
      "done 3200\n",
      "done 3300\n",
      "done 3400\n",
      "done 3500\n",
      "done 3600\n",
      "done 3700\n",
      "done 3800\n",
      "done 3900\n",
      "done 4000\n",
      "done 4100\n",
      "Removed 13 documents considered non-english.\n"
     ]
    }
   ],
   "source": [
    "df = remove_non_english_documents(df)\n",
    "#df = pd.read_csv('Datasets/english_tokens.csv')[['url', 'content', 'category', 'tokens_en', 'english:confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get labels\n",
    "# The labels array is a lookup for label (category) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Art': 6,\n",
      " 'Deviancy': 2,\n",
      " 'Drugs': 9,\n",
      " 'Encyclopedia and Knowledge': 12,\n",
      " 'Fake Identity and Hitmen': 8,\n",
      " 'Finance and Fraud': 0,\n",
      " 'Gambling': 10,\n",
      " 'Guns': 13,\n",
      " 'Hosting and Programming and Hacking': 3,\n",
      " 'Online Marketplace': 11,\n",
      " 'Other': 1,\n",
      " 'Porn': 5,\n",
      " 'Social': 7,\n",
      " 'Web Catalogue': 4}\n"
     ]
    }
   ],
   "source": [
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "def get_labels(d_frame):\n",
    "    data_frame = d_frame.copy(deep = True)\n",
    "    df_categories = data_frame.drop_duplicates(subset = 'category')\n",
    "    df_categories = df_categories['category']\n",
    "    \n",
    "    index = 0\n",
    "    for category in df_categories:\n",
    "        labels_index[category] = index\n",
    "        index += 1\n",
    "\n",
    "get_labels(df)\n",
    "pprint.pprint(labels_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the texts array with the raw content of every page\n",
    "# Fill the labels_index array with the category id on the index position of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Millionaire\\'s Coaching                           #    # \"If somebody '\n",
      " 'offers you an amazing opportunity but you are not sure you can do it, say '\n",
      " 'yes - then learn how to do it later!\"â€• Richard Branson  ## Mr.Millionaire '\n",
      " 'Presents  ## THE BINARY MILLIONAIRE METHOD  For Any Queries, Support or '\n",
      " \"Questions, Don't Hesitate to Email Me Directly and I will reply within 2 to \"\n",
      " '24 hours - MrMillionaireSupport@Protonmail.com OR  wickr: mrmillionairex OR '\n",
      " 'ICQ: 740011109\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b    ## '\n",
      " 'HOW WE MAKING $77,400--  ## $158,400 a DAY TRADING  ## BINARY OPTIONS IN 15 '\n",
      " \"MINUTES!!  ## AND HOW YOU CAN DO IT TOO !  ## 100's OF MIND BOGGLING  ## \"\n",
      " 'PROOFS HERE    HELLO HELLO HELLO  YOU FINALLY FOUND US !!!!  WELCOME TO OUR '\n",
      " 'SECRET WORLD !!!!  THIS IS \"THE BINARY MILLIONAIRE METHOD\" !!!!!!    __READ '\n",
      " 'EACH AND EVERY SINGLE WORD ON THIS PAGE TILL__  __THE LAST____ TO KNOW THE '\n",
      " 'SHOCKING $1,000,000 SECRET CLUB__  __SURPRISE ____AS A FREE BONUS. __    ## '\n",
      " '![](https://i.imgur.com/L7wdsLc.jpg)![](http://imageupper.com/g/?S120001009Z15668758681125727)    '\n",
      " '\\\\---&gt; WE ARE A SYNDICATE OF TRADERS WHO CASH IN HUNDREDS OF THOUSANDS OF '\n",
      " 'DOLLARS EVERYDAY PREDICTING WHETHER THE PRICE OF A CURRENCY WILL GO UP OR '\n",
      " 'DOWN TRADING SUPER SIMPLE BINARY OPTIONS!    \\\\---&gt; What is Binary '\n",
      " \"Options ? It is Like COIN TOSS : You predict whether a currency pair's Price \"\n",
      " 'Go either Up or Down..Like Heads or Tails...in a Time Frame like 1 minute or '\n",
      " '5 minutes or 15 minutes etc.If You say it will go up and after the timer '\n",
      " 'hits 0, its UP from where you opened the trade, then You make almost Double '\n",
      " 'the money    \\\\---&gt; NO PAST EXPERIENCE NEEDED.THE COACHING IS 100% NEWBIE '\n",
      " 'FRIENDLY and 100% LEGAL.TIME TO STOP DOING ILLEGAL SHIT NOW BECAUSE YOU CAN '\n",
      " 'MAKE WAY MORE WITH OUR LEGAL ETHICAL SYSTEM THAN YOU WILL EVER MAKE USING '\n",
      " 'UNETHICAL ILLEGAL PRACTICES    \\\\---&gt; NOW I WITH A SELECT FEW TRADERS USE '\n",
      " 'OUR GODLY STRATEGY TO PREDICT THAT GIVES ALMOST 95%  WIN RATE !    '\n",
      " '\\\\---&gt;What If You can WIN 95 coin toss out of 100 ? YESSSS...You will '\n",
      " 'Print Money on Demand !!!!    \\\\---&gt;I WILL TEACH EVERYTHING ON VIDEOS '\n",
      " 'WITH AAA+ EMAIL SUPPORT...NO LAME EBOOKS TO READ...WATCH THE ACTION LIVE ON '\n",
      " '7  VIDEOS BY MYSELF !!!    \\\\---&gt;NOW THE BIGGEST GREATEST NEWS : We USE A '\n",
      " 'Money Management Formula which helps us to TURN $1 into $100, $10 into $1000 '\n",
      " ', $100 into $10,000 and so on  that is 100 Times the amount in Just 16 '\n",
      " 'Trades ! ONLY I TEACH THIS    --&gt;NOW, I AM GONNA TEACH YOU THIS 100X '\n",
      " 'FORMULA AND THIS IS HOW YOU WILL GROW YOUR AND TOUCH YOUR FIRST MILLION ! '\n",
      " 'THIS IS LETHAL !!!    \\\\---&gt;ANOTHER AMAZING THING : I AM NOT SHITTING YOU '\n",
      " 'HERE...HENCE, THIS IS NOT SOME LAME EBOOK BUT ME SHOWING YOU LIVE HOW I '\n",
      " \"TRADE ON DEAD SERIOUS VIDEOS !!!    \\\\---&gt; YOU WILL SEE ME ''LIVE'' \"\n",
      " 'IMPLEMENTING THE STRATEGY AND MAKING REAL COLD HARD CASHHHH ON CAMERA !!!!! '\n",
      " 'EXPLAINING EVERYTHING WITH MY REAL VOICE AND SKILLS !!!!  '\n",
      " '![](https://i.imgur.com/uaGKlK1.jpg)\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b    '\n",
      " '\\\\---&gt; HERE ARE SOME LIVE VIDEO PROOFS OF ME TRADING FROM THE COURSE :  '\n",
      " '\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b  \\u200b  '\n",
      " 'Watch Video Proof 1 : https://youtu.be/CLUBmn14f8k  '\n",
      " '\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b  \\u200b  Watch Video Proof 2 : '\n",
      " 'https://youtu.be/3e-87X94vP4  \\u200b  \\\\---&gt; THIS IS THE SAME METHOD WE '\n",
      " 'USE EVERYDAY TO MAKE AS HIGH AS $9000, $46,800, $68,400, $72,000 AND '\n",
      " '$158,000 IN JUST 1 DAY OR EVEN JUST 1 TRADE SOMETIMES !!!!    **_* TONS OF '\n",
      " 'UNDENIABLE PROOFS BELOW *_**    \\\\---&gt; SEE YOURSELF TO BELIEVE...HERE ARE '\n",
      " 'THE PROOFS ON YOUR FACE OF HUNDREDS OF MY TRADES :    __(CHECK THE IMAGE '\n",
      " 'PROOFS LINKS ON GOOGLE CHROME OR ANY CLEAN NET BROWSER) __  \\u200b  _**1. '\n",
      " '****http://imageupper.com/g/?S120001005L15668715651129133**_    _**2\\\\. '\n",
      " 'http://imageupper.com/g/?S120001002R15259356531542989**_    **STUDENTS '\n",
      " 'PROOFS BELOW:**    **1. '\n",
      " 'http://imageupper.com/g/?S120001004Z15668720841121461**    **_2\\\\. '\n",
      " 'http://imageupper.com/g/?S120001007L15668724301122261_**    **100% POSITIVE '\n",
      " 'REVIEWS ON WALLSTREET MARKET AND EMPIRE MARKET BY OUR REAL STUDENTS '\n",
      " 'BELOW.LETS SEE WHAT THEY WANT TO SAY ABOUT OUR COACHING (100+ REVIEWS) '\n",
      " ':**    _**WALL STREET : '\n",
      " 'http://imageupper.com/g/?S110001003O1566872901112776**_    _**EMPIRE MARKET: '\n",
      " 'http://imageupper.com/g/?S120001003U1566872977112443**_    '\n",
      " 'y\\u200b\\u200b\\u200b\\u200bNow The Question is if I have made a Million '\n",
      " 'Dollars if I am teaching people to  do so right ?  \\u200b  So Below is the '\n",
      " 'Proof of WELL OVER a good MILLION DOLLARS right here ?   Exactly '\n",
      " '$1.430,786.04 ONE MILLION AND FOUR HUNDRED DOLLARS CASHED OUT from my '\n",
      " 'Bitcoins Wallet\\u200b     '\n",
      " 'https://www.blockchain.com/btc/address/1N86h2ELNni2cwYYKZQRtb3tDzJp8xbXpm   '\n",
      " '($519,561.43 cashed out on this one!!!)       '\n",
      " 'https://www.blockchain.com/btc/address/1NU1SWC8u7WF7F9ZsN99fwG5uZBcVE6C4m   '\n",
      " '($522,579.31 cashed out on this one too!!!)       '\n",
      " 'https://www.blockchain.com/btc/address/1KPvrjYu3S49dVaBeLyFDN5SJGYawaoXJB   '\n",
      " '($388,645.30 cashed out from that one too!)    '\n",
      " '![](https://i.imgur.com/ALzGsWu.jpg)     \\\\---&gt; ANOTHER KICKASS THING : '\n",
      " 'THIS IS A COACHING PROGRAM AND NOT SOME LAME GUIDE TO READ....THIS IS THE '\n",
      " 'ONLY PRODUCT ON TOR WHICH WILL PROVIDE YOU WITH UNLIMITED EMAIL SUPPORT TILL '\n",
      " 'YOU TOUCH $1,000,000 IN PROFITS !!! AFTER THAT, IT WILL BE $250,000 TO JOIN '\n",
      " 'THE SYNDICATE IF YOU ARE INTERESTED !  '\n",
      " '\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b  '\n",
      " '--&gt;BEING ABLE TO HAVE MY PERSONAL EMAIL, ICQ CHAT IS WORTH A MILLION '\n",
      " 'DOLLARâ€¦YOU WILL BE GETTING PERSONAL EMAIL SUPPORT FROM MEâ€¦HOWZ THAT ?    '\n",
      " '\\\\---&gt;I WILL SHOW LIVE TRADING VIDEOS IN THE COACHING WITH THE BEST '\n",
      " \"TRADES AS WELL AS THE WRONG TRADES ON CAMERA SO THAT YOU DON'T MAKE SILLY \"\n",
      " 'MISTAKES !!!    \\\\---&gt; SO WHY I AM SELLING THIS WHEN I CAN KEEP MAKING MY '\n",
      " 'OWN MONEY ?    \\\\---&gt; THE ANSWER : FIRST OF ALL, THIS IS NOT A CHEAP ASS '\n",
      " 'COACHING BUT WILL COST $25,000 TO GET IN !    \\\\---&gt;AFTER THE FIRST 10 '\n",
      " 'SLOTS, THE PRICE WILL GO UP TO $2999 AND THEN TO THE ORIGINAL $25,000    '\n",
      " '\\\\---&gt; AND NO !!!  THIS IS NOT SOME SHITTY SELLING TACTIC.YOU CAN NOT BUY '\n",
      " 'NOW AND COME BACK TO CHECK LATER.YOU ARE WELCOME !!!    \\\\---&gt; SO IF YOU '\n",
      " 'ARE STILL LOOKING AT $999, THEN GET ON FIRE COZ YOU HAVE BEEN A LUCKY '\n",
      " 'BASTARD TO HAVE ARRIVED ON THIS PAGE AT THE RIGHT TIME    \\\\---&gt; GET IN '\n",
      " 'NOW FOR CHEAPPPP INSTEAD OF COMING BACK AND BUYING THIS SAME COACHING FOR '\n",
      " '$25,000 EVEN THOUGH IT WILL STILL BE AN EXCELLENT DEAL    \\\\---&gt; IF YOU '\n",
      " 'SEND ME A COMPLETE REVIEW OF MY COACHING AFTER YOU GO THROUGH THE VIDEOS AND '\n",
      " 'USING THE STRATEGY, THEN I WILL GIVE YOU ANOTHER STRATEGY THAT WE WILL BE '\n",
      " 'LAUNCHING FOR $500-$2500 IN A FEW WHICH IS CALLED \"THE TURBO MILLIONAIRE '\n",
      " 'METHOD\" AS A FREE BONUS    \\\\---&gt; AGAIN, THE TMM is a GOD MODE STRATEGY '\n",
      " 'AND NOT SOME REHASHED SHIT    \\\\---&gt; THIS WILL BE A STRATEGY TO TRADE '\n",
      " 'SUPER FAST 1 MINUTE TRADES WITH A GODLY 90-95% WIN RATE !!!!!!!    \\\\---&gt; '\n",
      " 'SO , EMAIL ME FOR THE BONUS !!!!    **AND NOWWWWWWW !!!!!!!**    '\n",
      " '**!!!!!!!!!!!!! TIME FOR THE SURPRISE !!!!!!!!!!!!**    **\\\\----&gt; VIP '\n",
      " 'INVITE TO \"THE $1,000,000 CLUB\" &lt;\\\\-----**    **THIS IS WHERE WE GIVE YOU '\n",
      " 'HUGE FAMOUS HIGH TICKET PRODUCTS &amp; COACHINGS WORTH $1000-$10,000 THAT '\n",
      " 'ARE MAKING MILLIONAIRES EVERYDAYâ€¦â€¦FOR FREE !!! SOME OF THE TITLES YOU WILL '\n",
      " 'GET ARE :**    **Here Are Some Titles of the Products You will be getting '\n",
      " 'everyday :**  **1.Learn the Loophole to turn $250 in $50,000 in Just 2 '\n",
      " 'Months**    **2.HOW I WENT FROM BROKE TO BUYING A LAMBORGHINI**    **3.HOW I '\n",
      " 'FLOOD MY BANK ACCOUNT EVERY WEEK WITH A MINIMUM $25,000 ! **    **4.HOW TO '\n",
      " 'GO FROM BROKE TO 1 MILLION DOLLARS IN 18 MONTHS !**    **5.HOW TO. MAKE '\n",
      " '$22,200 A DAY WITHOUT A PRODUCT OR LIST **    **AND MANY MANY MOREâ€¦**    '\n",
      " '**THESE ARE THE THINGS YOU DESERVE TO KNOW AS YOU ARE ON YOUR WAY TO '\n",
      " 'BECOMING A MILLIONAIRE.**    **I CARE ABOUT MY STUDENTS SUCCESS AND THAT IS '\n",
      " 'THE MOST IMPORTANT THING !!! THATS ALL I HAVE TO SAY**    **\\\\--&gt; THERE '\n",
      " 'IS A REASON WHY AFTER HAVING 400+ STUDENTS, I HAVE 100% POSITIVE '\n",
      " 'REVIEWS...BECAUSE MY STUDENTS LOVE ME FOR CHANGING THEIR LIVES.**    '\n",
      " '**--&gt; SO YOU CAN CLOSE THIS PAGE AND GET BACK TO THE BORING NORMAL '\n",
      " 'LIFEâ€¦**    **ORâ€¦.**    **TAKE ACTION NOW, MAKE THE BEST DECISION OF YOUR '\n",
      " 'LIFE AND JOIN THE ELITE 1% OF THIS WORLD FULL OF ADVENTURES AND FILTHY '\n",
      " 'RICHESâ€¦YOUR CALL !!!**    **SEE YOU INSIDE !!!!!**    **PEACE...LOVE...MONEY '\n",
      " '!!!! **    \\u200b  [\\\\---&gt; ONLY 10 COACHING SPOTS AT $999 (NEXT AT $2999 '\n",
      " 'AND THEN AT $25,000...check how many spots left below title) &lt;\\\\---  '\n",
      " '\\u200b  _BUY NOW_    Fill the purchase Form :    Your e-mail:     ICQ or '\n",
      " 'Wickr     STEP 1 - Send **0.124** to the bitcoin address below. Contact me '\n",
      " 'if you dont know how to buy bitcoins. Make payment to the BTC Address below '\n",
      " ':  1NoorFjQfBgeZq7GEyjsXDYPBCT1uk9pp  STEP 2 - After Sending the Bitcoins '\n",
      " 'the the given Address : 1NoorFjQfBgeZq7GEyjsXDYPBCT1uk9pp , Fill the details '\n",
      " 'below and Submit them. After Your BTC Is confirmed, copy your transaction ID '\n",
      " 'and click the \"Verify and Download\" button and then download the '\n",
      " 'instructions to get started in your Email/ICQ/Wickr given above. (If you do '\n",
      " 'not know what is the blockchain hash, copy and paste the following picture '\n",
      " 'link into your browser and see it) https://i.imgur.com/2cZicwH.png        '\n",
      " 'for any queries,  Email at MrMillionaireSupport@Protonmail.com  OR  Wickr me '\n",
      " ': mrmillionairex_ _  _OR _  _ICQ: 740011109_  \\u200b  TIME TO CHANGE YOUR '\n",
      " 'LIFE HAS COME !!! SEE YOU INSIDE !!!\\u200b    '\n",
      " '![](https://i.imgur.com/DdvAUsp.jpg)    For Any Queries, Support or '\n",
      " 'Questions, Email Me Directly and I will reply with  2 to 24 hours -    '\n",
      " 'MrMillionaireSupport@gmail.com OR Wickr : mrmillionairex OR  ICQ: '\n",
      " '740011109\\u200b      '\n",
      " '![](https://static.wixstatic.com/media/3ca126_4f93ab42e3da4ea093027035ad0e357f~mv2.jpg/v1/fill/w_904,h_369,al_c,q_80,usm_0.66_1.00_0.01/3ca126_4f93ab42e3da4ea093027035ad0e357f~mv2.jpg)  '\n",
      " '![](https://static.wixstatic.com/media/3ca126_2c3fb8d8e2f349c29d59395499f877ed~mv2.jpg/v1/fill/w_391,h_586,al_c,q_80,usm_0.66_1.00_0.01/3ca126_2c3fb8d8e2f349c29d59395499f877ed~mv2.jpg)            ']\n",
      "[0, 1, 2, 3, 4]\n",
      "4107\n",
      "4107\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "def prepare_text_data(data_frame):\n",
    "    for i, document in data_frame.iterrows():\n",
    "        text = document['content']\n",
    "        texts.append(text)\n",
    "        category = document['category']\n",
    "        label = labels_index[category]\n",
    "        labels.append(label)\n",
    "\n",
    "prepare_text_data(df)\n",
    "\n",
    "pprint.pprint(texts[:1])    \n",
    "pprint.pprint(labels[:5])\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n",
      "        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n",
      "        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n",
      "        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n",
      "       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n",
      "        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n",
      "        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n",
      "        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n",
      "        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n",
      "       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n",
      "       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n",
      "       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n",
      "       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n",
      "        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n",
      "       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n",
      "        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n",
      "        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n",
      "       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n",
      "        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n",
      "       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "pprint.pprint(embeddings_index[\"hello\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nefarion/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 129766 unique tokens.\n",
      "Shape of data tensor: (4107, 1000)\n",
      "Shape of label tensor: (4107, 14)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"could not find the word {} in the embeddings dictionary\".format(word))\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "use_glove_embeddings = False #it seems that using glove embeddings doesn't bring up any noticeable improvement\n",
    "if use_glove_embeddings:\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "else:\n",
    "    embedding_layer = Embedding(num_words,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "14\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 2,246,542\n",
      "Trainable params: 2,246,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3286 samples, validate on 821 samples\n",
      "Epoch 1/20\n",
      "3286/3286 [==============================] - 25s 8ms/step - loss: 1.7113 - acc: 0.4601 - val_loss: 1.2997 - val_acc: 0.6261\n",
      "Epoch 2/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 1.1896 - acc: 0.6491 - val_loss: 1.0735 - val_acc: 0.7235\n",
      "Epoch 3/20\n",
      "3286/3286 [==============================] - 28s 8ms/step - loss: 0.9525 - acc: 0.7258 - val_loss: 0.9440 - val_acc: 0.7235\n",
      "Epoch 4/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 0.8460 - acc: 0.7635 - val_loss: 0.7684 - val_acc: 0.7844\n",
      "Epoch 5/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.7110 - acc: 0.8065 - val_loss: 0.7249 - val_acc: 0.7893\n",
      "Epoch 6/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.6436 - acc: 0.8168 - val_loss: 0.9036 - val_acc: 0.7686\n",
      "Epoch 7/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 0.5792 - acc: 0.8369 - val_loss: 0.7549 - val_acc: 0.7954\n",
      "Epoch 8/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 0.5264 - acc: 0.8466 - val_loss: 0.6596 - val_acc: 0.8112\n",
      "Epoch 9/20\n",
      "3286/3286 [==============================] - 28s 8ms/step - loss: 0.4813 - acc: 0.8603 - val_loss: 0.6662 - val_acc: 0.8295\n",
      "Epoch 10/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.4426 - acc: 0.8755 - val_loss: 0.6349 - val_acc: 0.8319\n",
      "Epoch 11/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.4026 - acc: 0.8871 - val_loss: 0.6370 - val_acc: 0.8307\n",
      "Epoch 12/20\n",
      "3286/3286 [==============================] - 26s 8ms/step - loss: 0.3654 - acc: 0.8965 - val_loss: 0.6077 - val_acc: 0.8453\n",
      "Epoch 13/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 0.3378 - acc: 0.9026 - val_loss: 0.6710 - val_acc: 0.8392\n",
      "Epoch 14/20\n",
      "3286/3286 [==============================] - 28s 8ms/step - loss: 0.3158 - acc: 0.9063 - val_loss: 0.6498 - val_acc: 0.8343\n",
      "Epoch 15/20\n",
      "3286/3286 [==============================] - 31s 9ms/step - loss: 0.2777 - acc: 0.9178 - val_loss: 0.6300 - val_acc: 0.8636\n",
      "Epoch 16/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.2655 - acc: 0.9224 - val_loss: 0.6349 - val_acc: 0.8502\n",
      "Epoch 17/20\n",
      "3286/3286 [==============================] - 28s 9ms/step - loss: 0.2200 - acc: 0.9422 - val_loss: 0.6513 - val_acc: 0.8551\n",
      "Epoch 18/20\n",
      "3286/3286 [==============================] - 28s 9ms/step - loss: 0.2105 - acc: 0.9385 - val_loss: 0.6944 - val_acc: 0.8551\n",
      "Epoch 19/20\n",
      "3286/3286 [==============================] - 27s 8ms/step - loss: 0.2147 - acc: 0.9407 - val_loss: 0.7298 - val_acc: 0.8563\n",
      "Epoch 20/20\n",
      "3286/3286 [==============================] - 29s 9ms/step - loss: 0.1652 - acc: 0.9522 - val_loss: 0.8573 - val_acc: 0.8575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb46b10bcf8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training model.')\n",
    "print(len(labels_index))\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data of 821 examples\n",
      "821/821 [==============================] - 2s 3ms/step\n",
      "test loss, test acc: [0.8573498459448913, 0.8574908375740051]\n",
      "\n",
      "# Detailed results for training data set\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Drugs should have been Encyclopedia and Knowledge\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Hosting and Programming and Hacking should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Guns\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Deviancy should have been Porn\n",
      "Predicted Deviancy should have been Other\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Fake Identity and Hitmen\n",
      "Predicted Finance and Fraud should have been Gambling\n",
      "Predicted Web Catalogue should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Web Catalogue should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Social should have been Guns\n",
      "Predicted Other should have been Web Catalogue\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Social should have been Other\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Social should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Web Catalogue should have been Gambling\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Deviancy should have been Other\n",
      "Predicted Other should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Encyclopedia and Knowledge\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Hosting and Programming and Hacking should have been Deviancy\n",
      "Predicted Porn should have been Online Marketplace\n",
      "Predicted Other should have been Web Catalogue\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Fake Identity and Hitmen\n",
      "Predicted Hosting and Programming and Hacking should have been Encyclopedia and Knowledge\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Hosting and Programming and Hacking should have been Web Catalogue\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Encyclopedia and Knowledge\n",
      "Predicted Web Catalogue should have been Gambling\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Fake Identity and Hitmen\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Other\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Other should have been Social\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Hosting and Programming and Hacking should have been Encyclopedia and Knowledge\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Fake Identity and Hitmen should have been Social\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Encyclopedia and Knowledge\n",
      "Predicted Finance and Fraud should have been Online Marketplace\n",
      "Predicted Finance and Fraud should have been Web Catalogue\n",
      "Predicted Finance and Fraud should have been Drugs\n",
      "Predicted Hosting and Programming and Hacking should have been Finance and Fraud\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Web Catalogue should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Porn should have been Hosting and Programming and Hacking\n",
      "Predicted Porn should have been Web Catalogue\n",
      "Predicted Porn should have been Deviancy\n",
      "Predicted Other should have been Drugs\n",
      "Predicted Hosting and Programming and Hacking should have been Social\n",
      "Predicted Social should have been Online Marketplace\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Hosting and Programming and Hacking should have been Other\n",
      "Predicted Social should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Other should have been Hosting and Programming and Hacking\n",
      "Predicted Finance and Fraud should have been Hosting and Programming and Hacking\n",
      "Predicted Hosting and Programming and Hacking should have been Drugs\n",
      "Predicted Social should have been Fake Identity and Hitmen\n",
      "Predicted Web Catalogue should have been Porn\n",
      "Predicted Finance and Fraud should have been Fake Identity and Hitmen\n",
      "Predicted Finance and Fraud should have been Social\n",
      "Predicted Hosting and Programming and Hacking should have been Art\n",
      "Predicted Hosting and Programming and Hacking should have been Fake Identity and Hitmen\n",
      "Predicted Web Catalogue should have been Other\n",
      "Predicted Porn should have been Other\n",
      "\n",
      "# Generate predictions for custom sample\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data of {} examples'.format(len(x_val)))\n",
    "results = model.evaluate(x_val, y_val, batch_size=128)\n",
    "run_accuracy = results[1]\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "category_id_to_name_lookup = {v: k for k, v in labels_index.items()}\n",
    "\n",
    "print('\\n# Detailed results for training data set')\n",
    "predictions = model.predict(x_val)\n",
    "predictions_copy = predictions.copy()\n",
    "for idx, val in enumerate(predictions):\n",
    "    category_index = predictions[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    if original_category_index != category_index:\n",
    "        print(\"Predicted {} should have been {}\"\n",
    "              .format(category_id_to_name_lookup[category_index], category_id_to_name_lookup[original_category_index]))\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for custom sample')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way â€” the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too â€” not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.\n",
      "\n",
      "# Saving model to disk\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Dump tokenizer so it can be used for tokenizing on unseen data with the same word dictionary.')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(\"\\n# Saving model to disk\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/DarkWebCategoryModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Models/DarkWebCategoryModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Test loaded model and tokenizer\n",
      "\n",
      "# Loading Tokenizer\n",
      "\n",
      "# Loading saved Model\n",
      "\n",
      "# Loading saved Model Weights\n",
      "\n",
      "# Compiling loaded Model\n",
      "Loading tokenizer, model, weights and compiling it took 0.7262263298034668 seconds.\n",
      "\n",
      "# Use loaded model on a custom exmaple\n",
      "Hosting and Programming and Hacking\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "print('\\n# Test loaded model and tokenizer')\n",
    "\n",
    "print('\\n# Loading Tokenizer')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "    \n",
    "print('\\n# Loading saved Model')\n",
    "from keras.models import model_from_json\n",
    "json_file = open('Models/DarkWebCategoryModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "print('\\n# Loading saved Model Weights')\n",
    "loaded_model.load_weights(\"Models/DarkWebCategoryModel.h5\")\n",
    "print('\\n# Compiling loaded Model')\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(\"Loading tokenizer, model, weights and compiling it took {} seconds.\".format(time.time() - start))\n",
    " \n",
    "print('\\n# Use loaded model on a custom exmaple')\n",
    "custom_example = [\"Tor Project | Research Tor Project | Research ![](https://research.torproject.org//images/tor- logo.svg)  About Documentation Support Blog Donate  __ Download Tor Browser  ###### Research  ## Home    * Home   * Safety Board   * Research Groups   * Ideas   * Tools   * Tech Reports   * Mailing Lists  Many people around the world are doing research on how to improve the Tor design, what's going on in the Tor network, and more generally on attacks and defenses for anonymous communication systems. This page summarizes the resources we provide to help make your Tor research more effective. You can reach us about research by picking one of the channels listed here.    * **Data**. We've been collecting data to learn more about the Tor network: how many relays and clients there are in the network, what capabilities they have, how fast the network is, how many clients are connecting via bridges, what traffic exits the network, etc. We are also developing tools to process these huge data archives and come up with useful statistics. Let us know what other information you'd like to see, and we can work with you to help make sure it gets collected safely and robustly.   * **Analysis**. If you're investigating Tor, or solving a Tor-related problem, _please_ talk to us somewhere along the way â€” the earlier the better. These days we review too many conference paper submissions that make bad assumptions and end up solving the wrong problem. Since the Tor protocol and the Tor network are both moving targets, measuring things without understanding what's going on behind the scenes is going to result in bad conclusions. In particular, different groups often unwittingly run a variety of experiments in parallel, and at the same time we're constantly modifying the design to try new approaches. If you let us know what you're doing and what you're trying to learn, we can help you understand what other variables to expect and how to interpret your results.   * **Measurement and attack tools**. We're building a repository of tools that can be used to measure, analyze, or perform attacks on Tor. Many research groups end up needing to do similar measurements (for example, change the Tor design in some way and then see if latency improves), and we hope to help everybody standardize on a few tools and then make them really good. Also, while there are some really neat Tor attacks that people have published about, it's hard to track down a copy of the code they used. Let us know if you have new tools we should list, or improvements to the existing ones. The more the better, at this stage.   * **We need defenses too â€” not just attacks**. Most researchers find it easy and fun to come up with novel attacks on anonymity systems. We've seen this result lately in terms of improved congestion attacks, attacks based on remotely measuring latency or throughput, and so on. Knowing how things can go wrong is important, and we recognize that the incentives in academia aren't aligned with spending energy on designing defenses, but it sure would be great to get more attention to how to address the attacks. We'd love to help brainstorm about how to make Tor better. As a bonus, your paper might even end up with a stroner section.   * **In-person help**. If you're doing interesting and important Tor research and need help understanding how the Tor network or design works, interpreting your data, crafting your experiments, etc, we can send a Tor researcher to your doorstep. As you might expect, we don't have a lot of free time; but making sure that research is done in a way that's useful to us is really important. So let us know, and we'll work something out.  If you're interested in anonymity research, you must make it to the Privacy Enhancing Technologies Symposium. Everybody who's anybody in the anonymity research world will be there. Stipends are generally available for people whose presence will benefit the community.  To get up to speed on anonymity research, read these papers (especially the ones in boxes). We also keep a list of Tor Tech Reports that are (co-)authored by Tor developers.  Our mission: to advance human rights and freedoms by creating and deploying free and open source anonymity and privacy technologies, supporting their unrestricted availability and use, and furthering their scientific and popular understanding.    * About    * Documentation    * Press    * Blog    * Newsletter    * Contact   Subscribe to our Newsletter  Get monthly updates and opportunities from the Tor Project:  Sign up  Trademark, copyright notices, and rules for use by third parties can be found in our _FAQ_.  \"]\n",
    "custom_sequences = loaded_tokenizer.texts_to_sequences(custom_example)\n",
    "predictions = loaded_model.predict(pad_sequences(custom_sequences, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "category_index = predictions[0].argmax(axis=0)\n",
    "print(category_id_to_name_lookup[category_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8574908375740051\n",
      "                                   : ['Art        ', 'Deviancy   ', 'Drugs      ', 'Encyklo    ', 'FakeIdnHitm', 'FinFraud   ', 'Gambling   ', 'Guns       ', 'HacProgr   ', 'OnlnMarket ', 'Other      ', 'Porn       ', 'Social     ', 'WebCat     ']\n",
      "\n",
      "Art                                : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 100.00% ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ']\n",
      "\n",
      "Deviancy                           : ['0: 0.00%   ', '17: 70.83% ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 4.17%   ', '0: 0.00%   ', '0: 0.00%   ', '6: 25.00%  ', '0: 0.00%   ', '0: 0.00%   ']\n",
      "\n",
      "Drugs                              : ['0: 0.00%   ', '0: 0.00%   ', '10: 76.92% ', '0: 0.00%   ', '0: 0.00%   ', '1: 7.69%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 7.69%   ', '0: 0.00%   ', '1: 7.69%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ']\n",
      "\n",
      "Encyclopedia and Knowledge         : ['0: 0.00%   ', '0: 0.00%   ', '1: 16.67%  ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '4: 66.67%  ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 16.67%  ', '0: 0.00%   ']\n",
      "\n",
      "Fake Identity and Hitmen           : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '7: 53.85%  ', '2: 15.38%  ', '0: 0.00%   ', '0: 0.00%   ', '3: 23.08%  ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 7.69%   ', '0: 0.00%   ']\n",
      "\n",
      "Finance and Fraud                  : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '144: 97.30%', '0: 0.00%   ', '0: 0.00%   ', '3: 2.03%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 0.68%   ', '0: 0.00%   ']\n",
      "\n",
      "Gambling                           : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 16.67%  ', '3: 50.00%  ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '2: 33.33%  ']\n",
      "\n",
      "Guns                               : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 50.00%  ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 50.00%  ', '0: 0.00%   ']\n",
      "\n",
      "Hosting and Programming and Hacking: ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '2: 2.25%   ', '0: 0.00%   ', '0: 0.00%   ', '79: 88.76% ', '0: 0.00%   ', '5: 5.62%   ', '1: 1.12%   ', '1: 1.12%   ', '1: 1.12%   ']\n",
      "\n",
      "Online Marketplace                 : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '7: 25.00%  ', '0: 0.00%   ', '0: 0.00%   ', '1: 3.57%   ', '14: 50.00% ', '0: 0.00%   ', '1: 3.57%   ', '1: 3.57%   ', '4: 14.29%  ']\n",
      "\n",
      "Other                              : ['0: 0.00%   ', '2: 1.43%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '3: 2.14%   ', '0: 0.00%   ', '0: 0.00%   ', '11: 7.86%  ', '0: 0.00%   ', '112: 80.00%', '1: 0.71%   ', '1: 0.71%   ', '10: 7.14%  ']\n",
      "\n",
      "Porn                               : ['0: 0.00%   ', '1: 0.46%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 0.46%   ', '0: 0.00%   ', '1: 0.46%   ', '208: 96.30%', '0: 0.00%   ', '5: 2.31%   ']\n",
      "\n",
      "Social                             : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '1: 5.26%   ', '5: 26.32%  ', '0: 0.00%   ', '0: 0.00%   ', '7: 36.84%  ', '0: 0.00%   ', '1: 5.26%   ', '0: 0.00%   ', '4: 21.05%  ', '1: 5.26%   ']\n",
      "\n",
      "Web Catalogue                      : ['0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '0: 0.00%   ', '4: 3.45%   ', '0: 0.00%   ', '0: 0.00%   ', '2: 1.72%   ', '0: 0.00%   ', '2: 1.72%   ', '2: 1.72%   ', '0: 0.00%   ', '106: 91.38%']\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = [cat_key for cat_key in labels_index]\n",
    "sorted_labels.sort()\n",
    "# Initiate confusion matrix\n",
    "matrix = {cat_key: {} for cat_key in sorted_labels}\n",
    "for row in matrix:\n",
    "    matrix[row] = {cat_key: 0 for cat_key in sorted_labels}\n",
    "\n",
    "len_of_longest = 0\n",
    "for label in sorted_labels:\n",
    "    label_len = len(label)\n",
    "    len_of_longest = label_len if label_len > len_of_longest else len_of_longest\n",
    "       \n",
    "# Create confusion matrix\n",
    "for idx, val in enumerate(predictions_copy):\n",
    "    category_index = predictions_copy[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    category = category_id_to_name_lookup[category_index]\n",
    "    original_category = category_id_to_name_lookup[original_category_index]\n",
    "    matrix[original_category][category] += 1\n",
    "\n",
    "# Simplify confusion matrix\n",
    "def get_percentage(res, values_sum):\n",
    "    return f'{res/values_sum*100:.2f}' if values_sum else '0.00'\n",
    "\n",
    "simple_matrix = matrix.copy()\n",
    "for row in simple_matrix:\n",
    "    values = simple_matrix[row].values()\n",
    "    values_sum = sum(values)\n",
    "    results_row = [ '{:11}'.format(f'{res}: {get_percentage(res, values_sum)}%') for res in list(values) ]\n",
    "    beau_results = [res.replace('\"', '') for res in results_row]\n",
    "    simple_matrix[row] = beau_results\n",
    "        \n",
    "# Print confusion matrix\n",
    "print(f'accuracy: {run_accuracy}')\n",
    "short_labels = ['OnlnMarket', 'WebCat', 'Porn', 'FinFraud', 'Other', 'Social', 'HacProgr', 'Encyklo', 'Deviancy', 'Drugs', 'FakeIdnHitm', 'Guns', 'Gambling', 'Art']\n",
    "short_labels.sort()\n",
    "formatted_short_labels = ['{:11}'.format(lab) for lab in short_labels]\n",
    "print(\"{:{len_of_longest}}: {}\".format(' ', formatted_short_labels, len_of_longest=len_of_longest))\n",
    "for row in simple_matrix:\n",
    "    print(\"\\n{:{len_of_longest}}: {}\".format(row, simple_matrix[row], len_of_longest=len_of_longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}