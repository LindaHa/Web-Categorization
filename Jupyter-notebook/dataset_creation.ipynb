{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "month = \"october\"\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "stopwords.extend('')\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "english_tolerance = 20\n",
    "english_confidence = []\n",
    "words_threshold = 10\n",
    "top = 2500\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "words_frequency = {}\n",
    "\n",
    "file = 'Datasets/URL-categorization-DFE.csv'\n",
    "df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']][:50]\n",
    "df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)]\n",
    "df['tokenized_words'] = ''\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    counter += 1\n",
    "    print(\"{}, {}/{}; Time: {}\".format(row['url'], counter, len(df), str(datetime.now())))\n",
    "    try:\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "           'Accept-Encoding': 'none',\n",
    "           'Accept-Language': 'en-US,en;q=0.8',\n",
    "           'Connection': 'keep-alive'}\n",
    "        req = urllib.request.Request('http://' + row['url'], headers=hdr)\n",
    "        html = urlopen(req, timeout=15).read()\n",
    "#             html = urlopen('http://' + row['url'], timeout=15).read()\n",
    "    except:\n",
    "        print(\"{} Failed\".format(row['url']))\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    [tag.decompose() for tag in soup(\"script\")]\n",
    "    [tag.decompose() for tag in soup(\"style\")]\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "    # Tokenize text\n",
    "    tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "    # Remove stopwords\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "    tokens_stopwords = [w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist]\n",
    "    tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_stopwords]\n",
    "    # Calculate percentage of english words\n",
    "    english_tokens = []\n",
    "    for word in tokens_lemmatize:\n",
    "        english_tokens.append(word.lower()) if word.lower() in english_vocab else ''\n",
    "    english_confidence = len(english_tokens) / len(tokens_lemmatize) * 100 if len(english_tokens) > 0 else 0\n",
    "    if len(english_tokens) < words_threshold or english_confidence < english_tolerance:\n",
    "        continue\n",
    "    # Track frequent words list for each category\n",
    "    if row['main_category'] not in [c for c in words_frequency.keys()]:\n",
    "        words_frequency[row['main_category']] = []\n",
    "    words_frequency[row['main_category']] = words_frequency[row['main_category']] + english_tokens\n",
    "\n",
    "    df.at[i, 'tokenized_words'] = english_tokens if english_confidence > english_tolerance else ''\n",
    "    df.at[i, 'english:confidence'] = english_confidence\n",
    "\n",
    "df = df[df['tokenized_words'] != '']\n",
    "df.to_csv(\"Datasets/full_data_{}_2.csv\".format(month))\n",
    "\n",
    "# Count words frequency for each category\n",
    "for cat in words_frequency.keys():\n",
    "    frequency = nltk.FreqDist(w for w in words_frequency[cat]).most_common(top)\n",
    "    words_frequency[cat] = [word for word, number in frequency]\n",
    "\n",
    "# Remove chunk words\n",
    "from math import floor\n",
    "words = []\n",
    "for category in words_frequency.keys():\n",
    "    words.extend(words_frequency[category][0:20])\n",
    "words_counter = Counter(words)\n",
    "chunk_words = [x for x in words_counter if words_counter[x] >= 7]\n",
    "words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 7}\n",
    "for cat in words_frequency.keys():\n",
    "    words_frequency[cat] = [word for word in words_frequency[cat] if word not in chunk_words]\n",
    "# Create labels and features set for ML\n",
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(row['tokenized_words']).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1\n",
    "# Features and labels splitting to training and testing data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train and validate data using ML algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print('DecisionTreeClassifier')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "\n",
    "#     # Save models\n",
    "#     from sklearn.externals import joblib\n",
    "#     filename = \"Models/LR_model_{}_2.joblib\".format(month)\n",
    "#     if not os.path.isfile(filename):\n",
    "#         joblib.dump(lr, filename)\n",
    "\n",
    "#     import pickle\n",
    "#     words_filename = \"Models/word_frequency_{}_2.picle\".format(month)\n",
    "#     if not os.path.isfile(words_filename):\n",
    "#         pickle_out = open(words_filename,\"wb\")\n",
    "#         pickle.dump(words_frequency, pickle_out)\n",
    "#         pickle_out.close()\n",
    "\n",
    "#     filename = \"Models/LR_maxtrain_{}_2.joblib\".format(month)\n",
    "#     if not os.path.isfile(filename):\n",
    "#         from sklearn.linear_model import LogisticRegression\n",
    "#         lr = LogisticRegression()\n",
    "#         lr.fit(X, y)\n",
    "#         joblib.dump(lr, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(row['tokenized_words']).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "month = \"october\"\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "english_tolerance = 50\n",
    "english_confidence = []\n",
    "words_threshold = 15\n",
    "top = 2500\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "words_frequency = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_filter_data():\n",
    "    file = 'Datasets/URL-categorization-DFE.csv'\n",
    "    df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "    df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)][:200]\n",
    "    df['tokenized_words'] = ''\n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        counter += 1\n",
    "        print(\"{}, {}/{}; Time: {}\".format(row['url'], counter, len(df), str(datetime.now())))\n",
    "        try:\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "               'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "               'Accept-Encoding': 'none',\n",
    "               'Accept-Language': 'en-US,en;q=0.8',\n",
    "               'Connection': 'keep-alive'}\n",
    "            req = urllib.request.Request('http://' + row['url'], headers=hdr)\n",
    "            html = urlopen(req, timeout=15).read()\n",
    "        except Exception as inst:\n",
    "            print(\"{} Failed. because of {}\".format(row['url'], inst))\n",
    "            continue\n",
    "\n",
    "        print(\"here\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        [tag.decompose() for tag in soup(\"script\")]\n",
    "        [tag.decompose() for tag in soup(\"style\")]\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        # Tokenize text\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        # Remove stopwords\n",
    "        wnl = WordNetLemmatizer()\n",
    "        tokens_stopwords = [w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist]\n",
    "        tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_stopwords]\n",
    "        # Calculate percentage of english words\n",
    "        english_tokens = []\n",
    "        for word in tokens_lemmatize:\n",
    "            if word.lower() in english_vocab:\n",
    "                english_tokens.append(word.lower())\n",
    "        english_confidence = len(english_tokens) / len(tokens_lemmatize) * 100 if len(english_tokens) > 0 else 0\n",
    "        if len(english_tokens) < words_threshold or english_confidence < english_tolerance:\n",
    "            continue\n",
    "        # Track frequent words list for each category\n",
    "        if row['main_category'] not in [c for c in words_frequency.keys()]:\n",
    "            words_frequency[row['main_category']] = []\n",
    "        words_frequency[row['main_category']] = words_frequency[row['main_category']] + english_tokens\n",
    "        \n",
    "        df.at[i, 'tokenized_words'] = english_tokens\n",
    "        df.at[i, 'english:confidence'] = english_confidence\n",
    "        \n",
    "    df = df[~df['tokenized_words'].isnull()]\n",
    "    df.to_csv(\"Datasets/full_data_{}.csv\".format(month))\n",
    "if not os.path.isfile(\"Datasets/full_data_{}.csv\".format(month)):\n",
    "    no_filter_data()\n",
    "df = pd.read_csv(\"Datasets/full_data_{}.csv\".format(month))\n",
    "df = df[df['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"Models/October/word_frequency_October.picle\",\"rb\")\n",
    "words_frequency = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words frequency for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "for cat in words_frequency.keys():\n",
    "    frequency = nltk.FreqDist(w.lower() for w in words_frequency[cat]).most_common(top)\n",
    "    words_frequency[cat] = [wnl.lemmatize(word) for word, number in frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove common words in all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "words = []\n",
    "for category in words_frequency.keys():\n",
    "    words.extend(words_frequency[category][0:20])\n",
    "words_counter = Counter(words)\n",
    "chunk_words = [x for x in words_counter if words_counter[x] >= 7]\n",
    "words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 7}\n",
    "for cat in words_frequency.keys():\n",
    "    words_frequency[cat] = [word for word in words_frequency[cat] if word not in chunk_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels and features set for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    print(counter) if row['url'] == '10bet.com' else ''\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df['main_category'].values))[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate data using ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print('DecisionTreeClassifier')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/October/LR_maxtrain_October.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-3c0ac6501216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Models/{}/SVM_model_{}.joblib\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/October/LR_maxtrain_October.joblib'"
     ]
    }
   ],
   "source": [
    "month = \"October\"\n",
    "from sklearn.externals import joblib\n",
    "filename = \"Models/Models_LR_model_{}.joblib\".format(month)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(lr, filename)\n",
    "\n",
    "filename = \"Models/{}/LR_maxtrain_{}.joblib\".format(month, month)\n",
    "if not os.path.isfile(filename):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr.fit(X, y)\n",
    "    joblib.dump(lr, filename)\n",
    "\n",
    "filename = \"Models/{}/SVM_model_{}.joblib\".format(month, month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(clf, filename)\n",
    "    \n",
    "import pickle\n",
    "words_filename = \"Models/{}/word_frequency_{}.picle\".format(month, month)\n",
    "if not os.path.isfile(words_filename):\n",
    "    pickle_out = open(words_filename,\"wb\")\n",
    "    pickle.dump(words_frequency, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
