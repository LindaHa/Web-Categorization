{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nefarion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import os.path\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/f/Linda/Work work/Categorization\")\n",
    "\n",
    "input_file_relative = \"Datasets/Dark_web_dataset.csv\"\n",
    "output_file_relative = \"Datasets/hopefully_the_end.csv\"\n",
    "\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 1000\n",
    "MAX_SEQUENCE_LENGTH = top\n",
    "MAX_NB_WORDS = top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read new generated data set file and set en_tokens to ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4334 rows in 4 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_file_relative)[['url', 'content', 'category']]\n",
    "df['tokens_en', 'confidence'] = ''\n",
    "# Shuffle the rows and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(f\"Loaded {df.shape[0]} rows in {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only English documents - above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_english_documents(data_frame, english_tolerance = 20):\n",
    "    removed = 0\n",
    "    english_confidence = []\n",
    "    tokens_en = []\n",
    "    for i, document in data_frame.iterrows():\n",
    "        english_words = 0\n",
    "        text = document['content']\n",
    "        \n",
    "        # Remove long base-64 encoded strings, e.g. images\n",
    "        text = re.sub(\"(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=|[A-Za-z0-9+/]{4}){24,}\",\"\", text)\n",
    "        \n",
    "        wordies = nltk.word_tokenize(text)\n",
    "\n",
    "        tokens = []\n",
    "        for w in wordies:\n",
    "            lower = w.lower()\n",
    "            if lower in english_vocab:\n",
    "                tokens.append(lower)\n",
    "                english_words += 1\n",
    "        tokens_en.append(tokens)\n",
    "        doc_english_confidence = english_words / len(wordies) * 100\n",
    "        english_confidence.append(doc_english_confidence)\n",
    "        if doc_english_confidence <= english_tolerance:\n",
    "            removed += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"done {}\".format(i))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    data_frame['english:confidence'] = english_confidence\n",
    "    data_frame['tokens_en'] = tokens_en\n",
    "    print(f\"Removed {removed} documents considered non-english.\")\n",
    "    return data_frame[data_frame['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0\n",
      "done 100\n",
      "done 200\n",
      "done 300\n",
      "done 400\n",
      "done 500\n",
      "done 600\n",
      "done 700\n",
      "done 800\n",
      "done 900\n",
      "done 1000\n",
      "done 1100\n",
      "done 1200\n",
      "done 1300\n",
      "done 1400\n",
      "done 1500\n",
      "done 1600\n",
      "done 1700\n",
      "done 1800\n",
      "done 1900\n",
      "done 2000\n",
      "done 2100\n",
      "done 2200\n",
      "done 2300\n",
      "done 2400\n",
      "done 2500\n",
      "done 2600\n",
      "done 2700\n",
      "done 2800\n",
      "done 2900\n",
      "done 3000\n",
      "done 3100\n",
      "done 3200\n",
      "done 3300\n"
     ]
    }
   ],
   "source": [
    "df = remove_non_english_documents(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get labels\n",
    "#### The labels array is a lookup for label (category) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "def get_labels(d_frame):\n",
    "    data_frame = d_frame.copy(deep = True)\n",
    "    df_categories = data_frame.drop_duplicates(subset = 'category')\n",
    "    df_categories = df_categories['category']\n",
    "    \n",
    "    index = 0\n",
    "    for category in df_categories:\n",
    "        labels_index[category] = index\n",
    "        index += 1\n",
    "\n",
    "get_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "#### Fill the texts array with the raw content of every page\n",
    "#### Fill the labels_index array with the category id on the index position of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "def prepare_text_data(data_frame):\n",
    "    for i, document in data_frame.iterrows():\n",
    "        text = document['content']\n",
    "        texts.append(text)\n",
    "        category = document['category']\n",
    "        label = labels_index[category]\n",
    "        labels.append(label)\n",
    "\n",
    "prepare_text_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Indexing word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Build an index of words mapping in the embeddings set to their embedding vector\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "print(num_words)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"could not find the word {} in the embeddings dictionary\".format(word))\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "use_glove_embeddings = True #it seems that using glove embeddings doesn't bring up any noticeable improvement\n",
    "if use_glove_embeddings:\n",
    "    input_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "else:\n",
    "    input_layer = Embedding(num_words,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare and train the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### prepare a 1D convnet with global maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = input_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the 1D convnet with global maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": false
   },
   "source": [
    "#### Evaluate the model on the test data using \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data of {} examples'.format(len(x_val)))\n",
    "results = model.evaluate(x_val, y_val, batch_size=128)\n",
    "run_accuracy = results[1]\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "category_id_to_name_lookup = {v: k for k, v in labels_index.items()}\n",
    "\n",
    "print('\\n# Detailed results for training data set')\n",
    "predictions = model.predict(x_val)\n",
    "predictions_copy = predictions.copy()\n",
    "for idx, val in enumerate(predictions):\n",
    "    category_index = predictions[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    if original_category_index != category_index:\n",
    "        print(\"Predicted {} should have been {}\"\n",
    "              .format(category_id_to_name_lookup[category_index], category_id_to_name_lookup[original_category_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dump reusable files: tokenizer.pickle, model.json, model.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n# Dump tokenizer so it can be used for tokenizing on other data with the same word dictionary.')\n",
    "import pickle\n",
    "with open('Models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print(\"\\n# Saving model to disk\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/DarkWebCategoryModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Models/DarkWebCategoryModel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a confusion matrix\n",
    "#### Prepare headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_labels = [cat_key for cat_key in labels_index]\n",
    "sorted_labels.sort()\n",
    "# Initiate confusion matrix\n",
    "matrix = {cat_key: {} for cat_key in sorted_labels}\n",
    "for row in matrix:\n",
    "    matrix[row] = {cat_key: 0 for cat_key in sorted_labels}\n",
    "\n",
    "len_of_longest = 0\n",
    "for label in sorted_labels:\n",
    "    label_len = len(label)\n",
    "    len_of_longest = label_len if label_len > len_of_longest else len_of_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%     \n"
    }
   },
   "outputs": [],
   "source": [
    "for idx, val in enumerate(predictions_copy):\n",
    "    category_index = predictions_copy[idx].argmax(axis=0)\n",
    "    original_category_index = y_val[idx].argmax(axis=0)\n",
    "    category = category_id_to_name_lookup[category_index]\n",
    "    original_category = category_id_to_name_lookup[original_category_index]\n",
    "    matrix[original_category][category] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplify the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%     \n"
    }
   },
   "outputs": [],
   "source": [
    "def get_percentage(res, values_sum):\n",
    "    return f'{res/values_sum*100:.2f}' if values_sum else '0.00'\n",
    "\n",
    "simple_matrix = matrix.copy()\n",
    "for row in simple_matrix:\n",
    "    values = simple_matrix[row].values()\n",
    "    values_sum = sum(values)\n",
    "    results_row = [ '{:11}'.format(f'{res}: {get_percentage(res, values_sum)}%') for res in list(values) ]\n",
    "    beau_results = [res.replace('\"', '') for res in results_row]\n",
    "    simple_matrix[row] = beau_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%       \n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'accuracy: {run_accuracy}')\n",
    "short_labels = ['OnlnMarket', 'WebCat', 'SexContent', 'FinFraud', 'Other', 'Social', 'HacProgr', 'Gambling', 'IllegalServ']\n",
    "short_labels.sort()\n",
    "formatted_short_labels = ['{:11}'.format(lab) for lab in short_labels]\n",
    "print(\"{:{len_of_longest}}, {}\".format(' ', formatted_short_labels, len_of_longest=len_of_longest))\n",
    "for row in simple_matrix:\n",
    "    print(\"\\n{:{len_of_longest}}, {}\".format(row, simple_matrix[row], len_of_longest=len_of_longest))\n",
    "\n",
    "print('category_lookup: ')\n",
    "pprint.pprint(category_id_to_name_lookup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}